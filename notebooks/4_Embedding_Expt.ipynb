{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be5fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "import numpy as np\n",
    "import random\n",
    "from haystack.nodes import FARMReader\n",
    "from haystack.document_stores.memory import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bec3287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhruv/.local/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configs\n",
    "\n",
    "## KG Config : To create them you need to run KG notebook in notebook folder\n",
    "KG_Graph_File = '../models/graph.pkl'\n",
    "KG_Embeddings_File = '../models/embeddings.pkl'\n",
    "#LM_model_path = \"deepset/roberta-base-squad2\"\n",
    "LM_model_path = 'DKud7/finetuned-roberta-squad2'\n",
    "reader = FARMReader(LM_model_path, use_gpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfdb5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_nodes(node, topn):\n",
    "    similar_nodes = embeddings.wv.most_similar(node, topn = topn)\n",
    "    return similar_nodes\n",
    "\n",
    "def get_content_from_embeddings(node, embeddings):\n",
    "    similar_nodes = embeddings.wv.most_similar(node, topn = 10)\n",
    "    content = str(node)\n",
    "    for n in similar_nodes:\n",
    "        content = content + ' ' + str(n[0])\n",
    "    return content\n",
    "\n",
    "def get_graph_and_embeddings(KG_Graph_File, KG_Embeddings_File):\n",
    "    with open(KG_Graph_File, 'rb') as f:\n",
    "        graph = pickle.load(f)\n",
    "    with open(KG_Embeddings_File, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return graph, embeddings\n",
    "\n",
    "def get_query_vector(query):\n",
    "    nodes_to_consider = []\n",
    "    for node in graph.nodes():\n",
    "        if node in query.lower():\n",
    "            nodes_to_consider.append(node)\n",
    "    total_nodes = len(graph.nodes())\n",
    "    query_vector = embeddings.wv[random.randint(0, total_nodes - 1)]\n",
    "    if len(nodes_to_consider):\n",
    "        query_vector = query_vector * 0\n",
    "        for node in nodes_to_consider:\n",
    "            query_vector = query_vector + embeddings.wv[node]\n",
    "        query_vector = query_vector / len(nodes_to_consider)\n",
    "    return query_vector\n",
    "\n",
    "def retrieve_docs_from_query_vector(query_vector, custom_documents, topk):\n",
    "    custom_documents.sort(key = lambda d: np.square(d['embedding'] - query_vector).mean())\n",
    "    return custom_documents[:topk]\n",
    "\n",
    "def get_answers_from_question(query, topk):\n",
    "    query_vector = get_query_vector(query)\n",
    "    returned_docs = retrieve_docs_from_query_vector(query_vector, custom_documents, topk)\n",
    "    document_store = InMemoryDocumentStore()\n",
    "    document_store.write_documents(returned_docs)\n",
    "    result = reader.predict(query = query, documents = document_store, top_k = topk)\n",
    "    return postprocess_answers(result, topk)\n",
    "\n",
    "def postprocess_answers(answers, top_k):\n",
    "    final_ans = []\n",
    "    graph, _ = get_graph_and_embeddings(KG_Graph_File, KG_Embeddings_File)\n",
    "    for ans in answers['answers']:\n",
    "        ans_seq = ans.answer\n",
    "        for node in graph.nodes():\n",
    "            if node in ans_seq and node not in final_ans:\n",
    "                final_ans.append(node)\n",
    "    return final_ans[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe66b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Creating custom docs from graph nodes and embeddings\n",
    "graph, embeddings = get_graph_and_embeddings(KG_Graph_File, KG_Embeddings_File)\n",
    "custom_documents = []\n",
    "for node in graph.nodes():\n",
    "    doc = {}\n",
    "    doc['node'] = node\n",
    "    doc['content'] = get_content_from_embeddings(node, embeddings)\n",
    "    doc['embedding'] = embeddings.wv[node]\n",
    "    custom_documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7597d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"pk released in which year?\"\n",
    "topk = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "275b940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.71s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sanjay dutt', 'rajkumar hirani', '3 idiots', 'her', 'forgetting sarah marshall', 'gran torino', 'ti', 'bee vang', 'ahney her', 'cynthia nixon']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answers = get_answers_from_question(query, topk)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68112649",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "226ab1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read 20 Questions from test file\n",
    "test_file = '../data/test.txt'\n",
    "answers_file = '../data/answers.txt'\n",
    "questions = []\n",
    "true_answers = []\n",
    "topk_predicted_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49ed450f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.50s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.53s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.82s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.60s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.83s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.71s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.79s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.70s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.68s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.71s/ Batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 35.0%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(test_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if i % 2 == 0 and i < 39:\n",
    "            questions.append(line.strip())\n",
    "        elif i > 41 and i < 62:\n",
    "            true_answers.append(line.strip()[8:-1].lower())\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Begin inference\n",
    "topk = 9\n",
    "\n",
    "# Write answers in file\n",
    "g = open(answers_file, 'w+')\n",
    "N = len(questions)\n",
    "acc = 0\n",
    "for i in range(N):\n",
    "    g.write('QUESTION: ' + questions[i] + '\\n')\n",
    "    g.write('TRUE ANSWER: ' + true_answers[i] + '\\n')\n",
    "    preds = get_answers_from_question(questions[i], topk)\n",
    "    g.write('PREDICTIONS: ' + ','.join(preds) + '\\n\\n')\n",
    "    if true_answers[i].lower() in preds:\n",
    "        acc = acc + 1\n",
    "\n",
    "print('ACCURACY: ' + str(acc * 100/N) + '%\\n')\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8399e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
